method: missforest
# Name of the method.

max_iter: 10
# int, optional (default = 10)
# The maximum iterations of the imputation process. Each column with a
# missing value is imputed exactly once in a given iteration.

decreasing: False
# boolean, optional (default = False)
# If set to True, columns are sorted according to decreasing number of
# missing values. In other words, imputation will move from imputing
# columns with the largest number of missing values to columns with
# fewest number of missing values.

# missing_values: np.nan
# integer, optional (default = np.nan)
# The placeholder for the missing values. All occurrences of
# `missing_values` will be imputed.

copy: True
# boolean, optional (default = True)
# If True, a copy of X will be created. If False, imputation will
# be done in-place whenever possible.

criterion: [squared_error, gini]
# tuple, optional (default = ('mse', 'gini'))
# The function to measure the quality of a split.The first element of
# the tuple is for the Random Forest Regressor (for imputing numerical
# variables) while the second element is for the Random Forest
# Classifier (for imputing categorical variables).

n_estimators: ${ml_params.n_estimators}
# integer, optional (default=100)
# The number of trees in the forest.

max_depth:
# integer or None, optional (default=None)
# The maximum depth of the tree. If None, then nodes are expanded until
# all leaves are pure or until all leaves contain less than
# min_samples_split samples.

min_samples_split: 2
# int, float, optional (default=2)
# The minimum number of samples required to split an internal node:
# - If int, then consider `min_samples_split` as the minimum number.
# - If float, then `min_samples_split` is a fraction and
#   `ceil(min_samples_split * n_samples)` are the minimum
#    number of samples for each split.

min_samples_leaf: 1
# int, float, optional (default=1)
# The minimum number of samples required to be at a leaf node.
# A split point at any depth will only be considered if it leaves at
# least ``min_samples_leaf`` training samples in each of the left and
# right branches.  This may have the effect of smoothing the model,
# especially in regression.
# - If int, then consider `min_samples_leaf` as the minimum number.
# - If float, then `min_samples_leaf` is a fraction and
#   `ceil(min_samples_leaf * n_samples)` are the minimum
#   number of samples for each node.

min_weight_fraction_leaf: 0.
# float, optional (default=0.)
# The minimum weighted fraction of the sum total of weights (of all
# the input samples) required to be at a leaf node. Samples have
# equal weight when sample_weight is not provided.

max_features:
# int, float, string or None, optional (default="auto")
# The number of features to consider when looking for the best split:
# - If int, then consider `max_features` features at each split.
# - If float, then `max_features` is a fraction and
#   `int(max_features * n_features)` features are considered at each
#    split.
# - If "auto", then `max_features=sqrt(n_features)`.
# - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
# - If "log2", then `max_features=log2(n_features)`.
# - If None, then `max_features=n_features`.
# Note: the search for a split does not stop until at least one
# valid partition of the node samples is found, even if it requires to
# effectively inspect more than ``max_features`` features.

max_leaf_nodes:
# int or None, optional (default=None)
# Grow trees with ``max_leaf_nodes`` in best-first fashion.
# Best nodes are defined as relative reduction in impurity.
# If None then unlimited number of leaf nodes.

min_impurity_decrease: 0.
# float, optional (default=0.)
# A node will be split if this split induces a decrease of the impurity
# greater than or equal to this value.
# The weighted impurity decrease equation is the following::
#   N_t / N * (impurity - N_t_R / N_t * right_impurity
#       - N_t_L / N_t * left_impurity)
#  where ``N`` is the total number of samples, ``N_t`` is the number of
#  samples at the current node, ``N_t_L`` is the number of samples in the
#  left child, and ``N_t_R`` is the number of samples in the right child.
#  ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
#  if ``sample_weight`` is passed.

bootstrap: True
# boolean, optional (default=True)
# Whether bootstrap samples are used when building trees.

oob_score: False
# bool (default=False)
# Whether to use out-of-bag samples to estimate
# the generalization accuracy.

n_jobs: -1
# int or None, optional (default=-1)
# The number of jobs to run in parallel for both `fit` and `predict`.
#  ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
#  ``-1`` means using all processors.

random_state: ${seed}
# int, RandomState instance or None, optional (default=None)
# If int, random_state is the seed used by the random number generator;
# If RandomState instance, random_state is the random number generator;
# If None, the random number generator is the RandomState instance used
# by `np.random`.

verbose: 0 # ${verbose}
# int, optional (default=0)
# Controls the verbosity when fitting and predicting.

# warm_start: False
# bool, optional (default=False)
# When set to ``True``, reuse the solution of the previous call to fit
# and add more estimators to the ensemble, otherwise, just fit a whole
# new forest. See :term:`the Glossary <warm_start>`.

class_weight: balanced
# dict, list of dicts, "balanced", "balanced_subsample" or None, optional (default=None)
# Weights associated with classes in the form ``{class_label: weight}``.
# If not given, all classes are supposed to have weight one. For
# multi-output problems, a list of dicts can be provided in the same
# order as the columns of y.
# Note that for multioutput (including multilabel) weights should be
# defined for each class of every column in its own dict. For example,
# for four-class multilabel classification weights should be
# [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
# [{1:1}, {2:5}, {3:1}, {4:1}].
# The "balanced" mode uses the values of y to automatically adjust
# weights inversely proportional to class frequencies in the input data
# as ``n_samples / (n_classes * np.bincount(y))``
# The "balanced_subsample" mode is the same as "balanced" except that
# weights are computed based on the bootstrap sample for every tree
# grown.
# For multi-output, the weights of each column of y will be multiplied.
# Note that these weights will be multiplied with sample_weight (passed
# through the fit method) if sample_weight is specified.
# NOTE: This parameter is only applicable for Random Forest Classifier
# objects (i.e., for categorical variables).

concat_mask: False # False
# A boolean value to indicate whether to concatenate the mask to the data or not.
# Default = False.